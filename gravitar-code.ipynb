{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gravitar-code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxr06wm3iQ0h"
      },
      "source": [
        "**Proximal Policy Optimization (PPO) + Noisy Layers for additional exploration**\r\n",
        "\r\n",
        "Reference code used:\r\n",
        "\r\n",
        "\r\n",
        "1.   https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb\r\n",
        "2.   https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/atari_ppo.py\r\n",
        "3. https://github.com/Shmuma/ptan\r\n",
        "4. https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py \r\n",
        "\r\n",
        "Side note: I followed \"Deep-Reinforcement-Learning-Hands-On-Second-Edition\" by Maxim Lapan. I constructed a few different algorithms proposed in the book (such as rainbow DQN, and A2C) - This algorithm while not being particularly sample efficient and quite slow to train acheived the best results for me.\r\n",
        "\r\n",
        "Training dynamics: the training was done over several days (approximately 100 hours) in multiple iterations to acheive a best mean score of 1447.00, this is of course reflected in the log file. 2850 was the best video recorded score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqTEB3q0iMZ8",
        "outputId": "7fb88608-75c0-40d5-aa0c-06d710467b5c"
      },
      "source": [
        "'''The training process is very slow so mount drive to save training checkpoints'''\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdKb_a8iypLY"
      },
      "source": [
        "**Main Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvZHVi-2AQri"
      },
      "source": [
        "'''Imports'''\r\n",
        "\r\n",
        "import gym\r\n",
        "from gym import spaces\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import collections\r\n",
        "from collections import deque\r\n",
        "import math\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.distributions import Categorical\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "'''Wrappers from openai atari baselines\r\n",
        "https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\r\n",
        "'''       \r\n",
        "class FireResetEnv(gym.Wrapper):\r\n",
        "  def __init__(self, env):\r\n",
        "    gym.Wrapper.__init__(self, env)\r\n",
        "    assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "    assert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "  def reset(self, **kwargs):\r\n",
        "    self.env.reset(**kwargs)\r\n",
        "    obs, _, done, _ = self.env.step(1)\r\n",
        "    if done:\r\n",
        "      self.env.reset(**kwargs)\r\n",
        "    obs, _, done, _ = self.env.step(2)\r\n",
        "    if done:\r\n",
        "      self.env.reset(**kwargs)\r\n",
        "    return obs\r\n",
        "\r\n",
        "  def step(self, ac):\r\n",
        "    return self.env.step(ac)\r\n",
        "\r\n",
        "class SkipEnv(gym.Wrapper):\r\n",
        "  def __init__(self, env, skip=4):\r\n",
        "    gym.Wrapper.__init__(self, env)\r\n",
        "    self._skip = skip\r\n",
        "\r\n",
        "  def step(self, action):\r\n",
        "    total_reward = 0.0\r\n",
        "    done = None\r\n",
        "    for i in range(self._skip):\r\n",
        "      obs, reward, done, info = self.env.step(action)\r\n",
        "      total_reward += reward\r\n",
        "      if done:\r\n",
        "        break\r\n",
        "\r\n",
        "    return obs, total_reward, done, info\r\n",
        "\r\n",
        "  def reset(self, **kwargs):\r\n",
        "    return self.env.reset(**kwargs)\r\n",
        "\r\n",
        "class EpisodicLifeEnv(gym.Wrapper):\r\n",
        "  def __init__(self, env):\r\n",
        "    gym.Wrapper.__init__(self, env)\r\n",
        "    self.lives = 0\r\n",
        "    self.was_real_done  = True\r\n",
        "\r\n",
        "  def step(self, action):\r\n",
        "    obs, reward, done, info = self.env.step(action)\r\n",
        "    self.was_real_done = done\r\n",
        "    lives = self.env.unwrapped.ale.lives()\r\n",
        "    if lives < self.lives and lives > 0:\r\n",
        "      done = True\r\n",
        "    self.lives = lives\r\n",
        "    return obs, reward, done, info\r\n",
        "\r\n",
        "  def reset(self, **kwargs):\r\n",
        "    if self.was_real_done:\r\n",
        "      obs = self.env.reset(**kwargs)\r\n",
        "    else:\r\n",
        "      obs, _, _, _ = self.env.step(0)\r\n",
        "    self.lives = self.env.unwrapped.ale.lives()\r\n",
        "    return obs\r\n",
        "\r\n",
        "class ClipRewardEnv(gym.RewardWrapper):\r\n",
        "  def __init__(self, env):\r\n",
        "    gym.RewardWrapper.__init__(self, env)\r\n",
        "\r\n",
        "  def reward(self, reward):\r\n",
        "    return np.sign(reward)\r\n",
        "\r\n",
        "'''Wrapper for marking output'''\r\n",
        "class MarkingWrapper(gym.Wrapper):\r\n",
        "  def __init__(self, env):\r\n",
        "    super(MarkingWrapper, self).__init__(env)\r\n",
        "    self.total_reward = 0.0\r\n",
        "\r\n",
        "  def step(self, action):\r\n",
        "    obs, reward, done, info = self.env.step(action)\r\n",
        "    self.total_reward += reward\r\n",
        "    if done:\r\n",
        "      marking.append(self.total_reward)\r\n",
        "      self.total_reward = 0\r\n",
        "    return obs, reward, done, info\r\n",
        "\r\n",
        "def make_env(env_name, episodic_life=True, reward_clipping=True, monitor=False):\r\n",
        "  env = gym.make(env_name)\r\n",
        "  env = MarkingWrapper(env)\r\n",
        "  if monitor:\r\n",
        "    env = gym.wrappers.Monitor(env, \"./video\",video_callable=lambda episode_id: episode_id, force=True)\r\n",
        "  env = SkipEnv(env, skip=4)\r\n",
        "  if episodic_life:\r\n",
        "    env = EpisodicLifeEnv(env)\r\n",
        "  env = FireResetEnv(env)\r\n",
        "  if reward_clipping:\r\n",
        "    env = ClipRewardEnv(env)\r\n",
        "  return env\r\n",
        "\r\n",
        "'''\r\n",
        "Give the weights nice starting values\r\n",
        "https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb\r\n",
        "'''\r\n",
        "def init_weights(m):\r\n",
        "  if isinstance(m, nn.Linear):\r\n",
        "    nn.init.normal_(m.weight, mean=0., std=0.1)\r\n",
        "    nn.init.constant_(m.bias, 0.1)\r\n",
        "\r\n",
        "'''\r\n",
        "Noisy layer\r\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/lib/dqn_extra.py\r\n",
        "'''\r\n",
        "class NoisyLinear(nn.Linear):\r\n",
        "  def __init__(self, in_features, out_features,\r\n",
        "                 sigma_init=0.017, bias=True):\r\n",
        "    super(NoisyLinear, self).__init__(\r\n",
        "            in_features, out_features, bias=bias)\r\n",
        "    w = torch.full((out_features, in_features), sigma_init)\r\n",
        "    self.sigma_weight = nn.Parameter(w)\r\n",
        "    z = torch.zeros(out_features, in_features)\r\n",
        "    self.register_buffer(\"epsilon_weight\", z)\r\n",
        "    if bias:\r\n",
        "      w = torch.full((out_features,), sigma_init)\r\n",
        "      self.sigma_bias = nn.Parameter(w)\r\n",
        "      z = torch.zeros(out_features)\r\n",
        "      self.register_buffer(\"epsilon_bias\", z)\r\n",
        "    self.reset_parameters()\r\n",
        "\r\n",
        "  def reset_parameters(self):\r\n",
        "    std = math.sqrt(3 / self.in_features)\r\n",
        "    self.weight.data.uniform_(-std, std)\r\n",
        "    self.bias.data.uniform_(-std, std)\r\n",
        "\r\n",
        "  def forward(self, input):\r\n",
        "    if not self.training:\r\n",
        "      return super(NoisyLinear, self).forward(input)\r\n",
        "    bias = self.bias\r\n",
        "    if bias is not None:\r\n",
        "      bias = bias + self.sigma_bias * \\\r\n",
        "            self.epsilon_bias.data\r\n",
        "    v = self.sigma_weight * self.epsilon_weight.data + \\\r\n",
        "      self.weight\r\n",
        "    return F.linear(input, v, bias)\r\n",
        "\r\n",
        "  def sample_noise(self):\r\n",
        "    self.epsilon_weight.normal_()\r\n",
        "    if self.bias is not None:\r\n",
        "      self.epsilon_bias.normal_()\r\n",
        "\r\n",
        "'''\r\n",
        "trainable network\r\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/lib/ppo.py\r\n",
        "'''\r\n",
        "class NoisyPPO(nn.Module):\r\n",
        "  def __init__(self, input_shape, n_actions):\r\n",
        "    super(NoisyPPO, self).__init__()\r\n",
        "\r\n",
        "    self.noisy_layers = [\r\n",
        "      NoisyLinear(np.array(input_shape).prod(), 256),\r\n",
        "      NoisyLinear(256, 84),\r\n",
        "      NoisyLinear(84, n_actions),\r\n",
        "    ]\r\n",
        "\r\n",
        "    # noisy layers for the policy network\r\n",
        "    self.actor = nn.Sequential(\r\n",
        "      self.noisy_layers[0],\r\n",
        "      nn.ReLU(),\r\n",
        "      self.noisy_layers[1],\r\n",
        "      nn.ReLU(),\r\n",
        "      self.noisy_layers[2],\r\n",
        "    )\r\n",
        "\r\n",
        "    # linear layers for the value network\r\n",
        "    self.critic = nn.Sequential(\r\n",
        "      nn.Linear(np.array(input_shape).prod(), 256),\r\n",
        "      nn.ReLU(),\r\n",
        "      nn.Linear(256, 84),\r\n",
        "      nn.ReLU(),\r\n",
        "      nn.Linear(84, 1)\r\n",
        "    )\r\n",
        "\r\n",
        "    self.apply(init_weights)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = x.view(x.size(0),-1)\r\n",
        "    fx = x.float() / 256\r\n",
        "    return self.actor(fx), self.critic(fx)\r\n",
        "\r\n",
        "  def sample_noise(self):\r\n",
        "    for l in self.noisy_layers:\r\n",
        "      l.sample_noise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjxZrpXb_j0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5ea59f-9f45-41fb-ef24-8de6f6f33863"
      },
      "source": [
        "'''params'''\r\n",
        "# use ram because it trains quicker\r\n",
        "ENV_NAME = \"Gravitar-ram-v0\"\r\n",
        "\r\n",
        "GAMMA = 0.99\r\n",
        "LEARNING_RATE = 1e-5\r\n",
        "ENTROPY_BETA = 0.01\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "PPO_TRAJ = 1025\r\n",
        "PPO_EPOCHS = 2\r\n",
        "\r\n",
        "GAE_LAMBDA = 0.95\r\n",
        "CLIP_GRAD = 0.2\r\n",
        "\r\n",
        "NUM_ENVS = 8\r\n",
        "\r\n",
        "seed = 742\r\n",
        "\r\n",
        "# change args to --val to record the trained model\r\n",
        "args = '--train'\r\n",
        "\r\n",
        "'''\r\n",
        "calculate gae\r\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/lib/ppo.py\r\n",
        "'''\r\n",
        "def calc_adv_ref(values, dones, rewards, gamma, gae_lambda):\r\n",
        "  last_gae = 0.0\r\n",
        "  adv, ref = [], []\r\n",
        "\r\n",
        "  for val, next_val, done, reward in zip(reversed(values[:-1]), reversed(values[1:]),\r\n",
        "                                           reversed(dones[:-1]), reversed(rewards[:-1])):\r\n",
        "    if done:\r\n",
        "      delta = reward - val\r\n",
        "      last_gae = delta\r\n",
        "    else:\r\n",
        "      delta = reward + gamma * next_val - val\r\n",
        "      last_gae = delta + gamma * gae_lambda * last_gae\r\n",
        "    adv.append(last_gae)\r\n",
        "    ref.append(last_gae + val)\r\n",
        "  adv = list(reversed(adv))\r\n",
        "  ref = list(reversed(ref))\r\n",
        "  return torch.FloatTensor(adv), torch.FloatTensor(ref)\r\n",
        "\r\n",
        "'''\r\n",
        "train on ppo batch\r\n",
        "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/atari_ppo.py\r\n",
        "'''\r\n",
        "def train(batch):\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    states_t, actions_t, advantage, ref, old_logprob = batch\r\n",
        "    policy, value = net(states_t)\r\n",
        "    loss_value = F.mse_loss(value.squeeze(-1), ref)\r\n",
        "\r\n",
        "    logpolicy = F.log_softmax(policy, dim=1)\r\n",
        "\r\n",
        "    probs = F.softmax(policy, dim=1)\r\n",
        "    loss_entropy = (probs * logpolicy).sum(dim=1).mean()\r\n",
        "\r\n",
        "    logprob = logpolicy.gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\r\n",
        "    ratio = torch.exp(logprob - old_logprob)\r\n",
        "    surr_obj = advantage * ratio\r\n",
        "    clipped_surr = advantage * torch.clamp(ratio, 1.0 - CLIP_GRAD, 1.0 + CLIP_GRAD)\r\n",
        "    loss_policy = -torch.min(surr_obj, clipped_surr).mean()\r\n",
        "\r\n",
        "    loss = ENTROPY_BETA * loss_entropy + loss_policy + loss_value\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "# setup CUDA\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\r\n",
        "\r\n",
        "# setup multiple environments and set seeds\r\n",
        "envs = [make_env(ENV_NAME) for i in range(NUM_ENVS)]\r\n",
        "for idx, env in enumerate(envs):\r\n",
        "  env.seed(seed + idx)\r\n",
        "  env.action_space.seed(seed + idx)\r\n",
        "\r\n",
        "# set seeds\r\n",
        "torch.manual_seed(seed)\r\n",
        "torch.cuda.manual_seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "random.seed(seed)\r\n",
        "torch.manual_seed(seed)\r\n",
        "\r\n",
        "# init networks and optimizers\r\n",
        "net = NoisyPPO(env.observation_space.shape, env.action_space.n).to(device)\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "# additional variables for monitoring progress\r\n",
        "n_episode = 0\r\n",
        "marking = collections.deque(maxlen=100)\r\n",
        "\r\n",
        "frame_idx = 0\r\n",
        "best_m_reward = None\r\n",
        "\r\n",
        "'''try to load a checkpoint save otherwise start from the begining'''\r\n",
        "try:\r\n",
        "  params = torch.load('drive/My Drive/training/gravitar-save.chkpt')\r\n",
        "  net.load_state_dict(params['net'])\r\n",
        "  optimizer.load_state_dict(params['optimizer'])\r\n",
        "  n_episode = params['n_episode']\r\n",
        "  best_m_reward = params['best_m_reward']\r\n",
        "  print(\"Resuming training session from gravitar-save.chkpt\")\r\n",
        "except:\r\n",
        "  print(\"Starting new training session\")\r\n",
        "\r\n",
        "if args == '--val':\r\n",
        "\r\n",
        "  # setup validation environment with video recording\r\n",
        "  env = make_env(ENV_NAME, episodic_life=False, reward_clipping=False, monitor=True)\r\n",
        "  env.seed(seed)\r\n",
        "  env.action_space.seed(seed)\r\n",
        "  \r\n",
        "  # run 100 episodes and record videos for each\r\n",
        "  for i in range(100):\r\n",
        "    state = env.reset()\r\n",
        "    done = False\r\n",
        "    while not done:\r\n",
        "      with torch.no_grad():\r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\r\n",
        "        logits, _ = net(state)\r\n",
        "\r\n",
        "        probs = F.softmax(logits, dim=1)\r\n",
        "        dist = Categorical(probs)\r\n",
        "\r\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\r\n",
        "        state = next_state\r\n",
        "\r\n",
        "    print(\"episode {}, score {:.1f}\".format(i, marking[-1]))\r\n",
        "\r\n",
        "  print(\"validation episodes: {}, mean_score: {:.2f}, std_score: {:.2f}\".format(\r\n",
        "                100,  np.array(marking).mean(), np.array(marking).std()))\r\n",
        "\r\n",
        "elif args == '--train':\r\n",
        "\r\n",
        "  # training loop\r\n",
        "  while True:\r\n",
        "\r\n",
        "    states = []\r\n",
        "    actions = []\r\n",
        "    rewards = []\r\n",
        "    dones = []\r\n",
        "    last_done_index = None\r\n",
        "\r\n",
        "    # interact with the environments in a round robin fashion\r\n",
        "    for e_idx, e in enumerate(envs):\r\n",
        "      state = e.reset()\r\n",
        "\r\n",
        "      # play an entire PPO trajectory in the current environment\r\n",
        "      for _ in range(PPO_TRAJ):\r\n",
        "        '''\r\n",
        "        batch generation\r\n",
        "        https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter21/lib/ppo.py\r\n",
        "        '''\r\n",
        "        with torch.no_grad():\r\n",
        "          state = torch.from_numpy(state).float().unsqueeze(0).to(device)\r\n",
        "          logits, _, = net(state)\r\n",
        "\r\n",
        "          probs = F.softmax(logits, dim=1)\r\n",
        "          dist = Categorical(probs)\r\n",
        "\r\n",
        "          action = dist.sample()\r\n",
        "\r\n",
        "          next_state, reward, done, _ = e.step(action.cpu().numpy())\r\n",
        "\r\n",
        "          states.append(state)\r\n",
        "          actions.append(action)\r\n",
        "          rewards.append(reward)\r\n",
        "          dones.append(done)\r\n",
        "\r\n",
        "          state = next_state\r\n",
        "\r\n",
        "          if done:\r\n",
        "            last_done_index = len(states)-1\r\n",
        "            state = e.reset()\r\n",
        "\r\n",
        "        frame_idx += 1\r\n",
        "        '''\r\n",
        "        training is very slow so we only print every 1 million frames\r\n",
        "        change to len(marking) == 100 to print every 100 episodes instead\r\n",
        "        '''\r\n",
        "        if frame_idx%1000000==0:\r\n",
        "          # get the mean reward for the last 100 episodes\r\n",
        "          m_reward = np.array(marking).mean()\r\n",
        "          n_episode += 100\r\n",
        "\r\n",
        "          if best_m_reward is None or best_m_reward < m_reward:\r\n",
        "            print(\"NEW BEST MEAN REWARD --> {:.2f}\".format(m_reward))\r\n",
        "            best_m_reward = m_reward\r\n",
        "\r\n",
        "          print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\r\n",
        "                n_episode, marking[-1], m_reward, np.array(marking).std()))\r\n",
        "          \r\n",
        "          # save the training for later - comment out if drive is not mounted\r\n",
        "          torch.save({'net':net.state_dict(), 'optimizer':optimizer.state_dict(), 'n_episode':n_episode, \r\n",
        "                      'best_m_reward':best_m_reward}, 'drive/My Drive/training/gravitar-save.chkpt')\r\n",
        "          \r\n",
        "          marking = collections.deque(maxlen=100)\r\n",
        "\r\n",
        "        # make sure we have a full trajectory\r\n",
        "        if len(states) < PPO_TRAJ:\r\n",
        "          continue\r\n",
        "\r\n",
        "        # make sure there is atleast one full episode in our trajectory\r\n",
        "        # improves training stability\r\n",
        "        if last_done_index is None or last_done_index == len(states)-1:\r\n",
        "          continue\r\n",
        "\r\n",
        "        # crop the trajectory removing transitions in incomplete episodes\r\n",
        "        # improves training stability\r\n",
        "        net.sample_noise()\r\n",
        "\r\n",
        "        states = states[:last_done_index+2]\r\n",
        "        actions = actions[:last_done_index + 2]\r\n",
        "        rewards = rewards[:last_done_index + 2]\r\n",
        "        dones = dones[:last_done_index + 2]\r\n",
        "\r\n",
        "        '''prepare the rest of the training batch'''\r\n",
        "\r\n",
        "        states_t = torch.cat(states).to(device)\r\n",
        "        actions_t = torch.tensor(actions).to(device)\r\n",
        "        policy, values = net(states_t)\r\n",
        "        values = values.squeeze()\r\n",
        "\r\n",
        "        advantage, ref = calc_adv_ref(values.data.cpu().numpy(),\r\n",
        "                                      dones, rewards, GAMMA, GAE_LAMBDA)\r\n",
        "        advantage = advantage.to(device)\r\n",
        "        ref = ref.to(device)\r\n",
        "\r\n",
        "        logpolicy = F.log_softmax(policy, dim=1)\r\n",
        "        old_logprob = logpolicy.gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\r\n",
        "        advantage = (advantage - torch.mean(advantage)) / torch.std(advantage)\r\n",
        "        old_logprob = old_logprob.detach()\r\n",
        "\r\n",
        "        # make our trajectory splittable on even batch chunks\r\n",
        "        len_trajectory = len(states_t) - 1\r\n",
        "        len_trajectory -= len_trajectory % BATCH_SIZE\r\n",
        "        len_trajectory += 1\r\n",
        "        indices = np.arange(0, len_trajectory-1)\r\n",
        "\r\n",
        "        # train for ppo epoches\r\n",
        "        for _ in range(PPO_EPOCHS):\r\n",
        "          np.random.shuffle(indices)\r\n",
        "          for batch_indices in np.split(indices, len_trajectory // BATCH_SIZE):\r\n",
        "            train((\r\n",
        "                          states_t[batch_indices],\r\n",
        "                          actions_t[batch_indices],\r\n",
        "                          advantage[batch_indices],\r\n",
        "                          ref[batch_indices],\r\n",
        "                          old_logprob[batch_indices],\r\n",
        "                      ))\r\n",
        "            \r\n",
        "        states.clear()\r\n",
        "        actions.clear()\r\n",
        "        rewards.clear()\r\n",
        "        dones.clear()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resuming training session from gravitar-save.chkpt\n",
            "episode 0, score 1850.0\n",
            "episode 1, score 1300.0\n",
            "episode 2, score 550.0\n",
            "episode 3, score 350.0\n",
            "episode 4, score 950.0\n",
            "episode 5, score 2050.0\n",
            "episode 6, score 1000.0\n",
            "episode 7, score 1850.0\n",
            "episode 8, score 600.0\n",
            "episode 9, score 250.0\n",
            "episode 10, score 800.0\n",
            "episode 11, score 1750.0\n",
            "episode 12, score 500.0\n",
            "episode 13, score 2450.0\n",
            "episode 14, score 1250.0\n",
            "episode 15, score 1150.0\n",
            "episode 16, score 800.0\n",
            "episode 17, score 1950.0\n",
            "episode 18, score 900.0\n",
            "episode 19, score 450.0\n",
            "episode 20, score 1750.0\n",
            "episode 21, score 900.0\n",
            "episode 22, score 2050.0\n",
            "episode 23, score 1250.0\n",
            "episode 24, score 1500.0\n",
            "episode 25, score 900.0\n",
            "episode 26, score 700.0\n",
            "episode 27, score 1850.0\n",
            "episode 28, score 2250.0\n",
            "episode 29, score 1300.0\n",
            "episode 30, score 2750.0\n",
            "episode 31, score 900.0\n",
            "episode 32, score 1750.0\n",
            "episode 33, score 1550.0\n",
            "episode 34, score 1300.0\n",
            "episode 35, score 800.0\n",
            "episode 36, score 2100.0\n",
            "episode 37, score 1450.0\n",
            "episode 38, score 1000.0\n",
            "episode 39, score 750.0\n",
            "episode 40, score 250.0\n",
            "episode 41, score 1150.0\n",
            "episode 42, score 1550.0\n",
            "episode 43, score 2750.0\n",
            "episode 44, score 350.0\n",
            "episode 45, score 1650.0\n",
            "episode 46, score 1300.0\n",
            "episode 47, score 700.0\n",
            "episode 48, score 1950.0\n",
            "episode 49, score 1350.0\n",
            "episode 50, score 2150.0\n",
            "episode 51, score 800.0\n",
            "episode 52, score 1650.0\n",
            "episode 53, score 900.0\n",
            "episode 54, score 100.0\n",
            "episode 55, score 1750.0\n",
            "episode 56, score 1350.0\n",
            "episode 57, score 1350.0\n",
            "episode 58, score 1550.0\n",
            "episode 59, score 1600.0\n",
            "episode 60, score 800.0\n",
            "episode 61, score 2150.0\n",
            "episode 62, score 1350.0\n",
            "episode 63, score 0.0\n",
            "episode 64, score 2250.0\n",
            "episode 65, score 2250.0\n",
            "episode 66, score 1200.0\n",
            "episode 67, score 1100.0\n",
            "episode 68, score 700.0\n",
            "episode 69, score 900.0\n",
            "episode 70, score 0.0\n",
            "episode 71, score 1000.0\n",
            "episode 72, score 1950.0\n",
            "episode 73, score 1350.0\n",
            "episode 74, score 800.0\n",
            "episode 75, score 1750.0\n",
            "episode 76, score 1200.0\n",
            "episode 77, score 1000.0\n",
            "episode 78, score 700.0\n",
            "episode 79, score 450.0\n",
            "episode 80, score 1550.0\n",
            "episode 81, score 800.0\n",
            "episode 82, score 1350.0\n",
            "episode 83, score 1400.0\n",
            "episode 84, score 1150.0\n",
            "episode 85, score 1350.0\n",
            "episode 86, score 900.0\n",
            "episode 87, score 2350.0\n",
            "episode 88, score 1650.0\n",
            "episode 89, score 1650.0\n",
            "episode 90, score 800.0\n",
            "episode 91, score 1850.0\n",
            "episode 92, score 1100.0\n",
            "episode 93, score 1650.0\n",
            "episode 94, score 1650.0\n",
            "episode 95, score 450.0\n",
            "episode 96, score 1800.0\n",
            "episode 97, score 1000.0\n",
            "episode 98, score 1950.0\n",
            "episode 99, score 2350.0\n",
            "validation episodes: 100, mean_score: 1294.00, std_score: 610.50\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}